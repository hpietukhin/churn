{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    learning_curve,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# XGBoost import\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {},
   "source": [
    "# Bank Customer Churn Prediction (Enhanced)\n",
    "\n",
    "This notebook implements a machine learning pipeline to predict customer churn\n",
    "using multiple classification algorithms.\n",
    "\n",
    "## Project Overview\n",
    "- **Dataset**: Bank Customer Churn Dataset (10,000 customers)\n",
    "- **Goal**: Predict whether a customer will leave the bank\n",
    "- **Methods**: Random Forest, AdaBoost, XGBoost, SVM, Logistic Regression\n",
    "- **Metrics**: Accuracy, Sensitivity, Specificity, Precision, F1-score, AUC, Gini\n",
    "\n",
    "## Enhancements over Original\n",
    "- **70/15/15 Train/Validation/Test split** (was 80/20 train/test)\n",
    "- **XGBoost** with early stopping on validation set\n",
    "- **SVM** with RBF kernel\n",
    "- **Feature engineering**: balance_to_salary_ratio\n",
    "- **VIF analysis** for multicollinearity detection\n",
    "- **Learning curves** for bias/variance diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 1. Data Loading\n",
       "\n",
       "Loaded dataset with **10000** rows and **11** columns.\n",
       "\n",
       "### Data Preview:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data (remove first column - customer_id)\n",
    "data_bank = pd.read_csv(\"public/Bank Customer Churn Prediction.csv\").iloc[\n",
    "    :, 1:\n",
    "]\n",
    "\n",
    "mo.md(f\"\"\"\n",
    "## 1. Data Loading\n",
    "\n",
    "Loaded dataset with **{len(data_bank)}** rows and **{len(data_bank.columns)}** columns.\n",
    "\n",
    "### Data Preview:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_marimo_row_id</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>products_number</th>\n",
       "      <th>credit_card</th>\n",
       "      <th>active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>645</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>822</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>376</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>501</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>684</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>134603.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71725.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.ui.table(data_bank.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Data Structure:\n",
       "\n",
       "**Columns**: credit_score, country, gender, age, tenure, balance, products_number, credit_card, active_member, estimated_salary, churn\n",
       "\n",
       "**Data Types**:\n",
       "```\n",
       "credit_score          int64\n",
       "country              object\n",
       "gender               object\n",
       "age                   int64\n",
       "tenure                int64\n",
       "balance             float64\n",
       "products_number       int64\n",
       "credit_card           int64\n",
       "active_member         int64\n",
       "estimated_salary    float64\n",
       "churn                 int64\n",
       "```\n",
       "\n",
       "**Missing Values**: 0\n",
       "\n",
       "**Target Distribution**:\n",
       "- Churn = 0 (stayed): 7963 (79.6%)\n",
       "- Churn = 1 (left): 2037 (20.4%)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.md(f\"\"\"\n",
    "### Data Structure:\n",
    "\n",
    "**Columns**: {\", \".join(data_bank.columns.tolist())}\n",
    "\n",
    "**Data Types**:\n",
    "```\n",
    "{data_bank.dtypes.to_string()}\n",
    "```\n",
    "\n",
    "**Missing Values**: {data_bank.isnull().sum().sum()}\n",
    "\n",
    "**Target Distribution**:\n",
    "- Churn = 0 (stayed): {(data_bank[\"churn\"] == 0).sum()} ({(data_bank[\"churn\"] == 0).sum() / len(data_bank) * 100:.1f}%)\n",
    "- Churn = 1 (left): {(data_bank[\"churn\"] == 1).sum()} ({(data_bank[\"churn\"] == 1).sum() / len(data_bank) * 100:.1f}%)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 2. Correlation Analysis\n",
       "\n",
       "Examining correlations between numeric variables and the target (churn).\n",
       "The three least correlated variables with churn are:\n",
       "- tenure (length of membership)\n",
       "- credit_card\n",
       "- estimated_salary"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correlation matrix for numeric variables\n",
    "num_data = data_bank.select_dtypes(include=[np.number])\n",
    "cor_matrix = num_data.corr()\n",
    "\n",
    "mo.md(\"\"\"\n",
    "## 2. Correlation Analysis\n",
    "\n",
    "Examining correlations between numeric variables and the target (churn).\n",
    "The three least correlated variables with churn are:\n",
    "- tenure (length of membership)\n",
    "- credit_card\n",
    "- estimated_salary\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix\n",
    "fig_corr, ax_corr = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cor_matrix,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=1,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    "    ax=ax_corr,\n",
    ")\n",
    "ax_corr.set_title(\n",
    "    \"Correlation Matrix of Numeric Variables\", fontsize=14, pad=20\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {},
   "source": [
    "## 3. Data Visualization\n",
    "\n",
    "Exploring the distribution of key variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution\n",
    "fig_age, ax_age = plt.subplots(figsize=(10, 6))\n",
    "ax_age.hist(data_bank[\"age\"], bins=20, color=\"pink\", edgecolor=\"black\")\n",
    "ax_age.set_title(\"Age Distribution of Customers\", fontsize=14)\n",
    "ax_age.set_xlabel(\"Age\")\n",
    "ax_age.set_ylabel(\"Count\")\n",
    "ax_age.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance distribution\n",
    "fig_balance, ax_balance = plt.subplots(figsize=(10, 6))\n",
    "ax_balance.hist(\n",
    "    data_bank[\"balance\"], bins=30, color=\"yellow\", edgecolor=\"black\"\n",
    ")\n",
    "ax_balance.set_title(\"Account Balance Distribution\", fontsize=14)\n",
    "ax_balance.set_xlabel(\"Balance\")\n",
    "ax_balance.set_ylabel(\"Count\")\n",
    "ax_balance.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenure distribution\n",
    "fig_tenure, ax_tenure = plt.subplots(figsize=(10, 6))\n",
    "ax_tenure.hist(data_bank[\"tenure\"], bins=10, color=\"purple\", edgecolor=\"black\")\n",
    "ax_tenure.set_title(\"Length of Membership in Bank\", fontsize=14)\n",
    "ax_tenure.set_xlabel(\"Years\")\n",
    "ax_tenure.set_ylabel(\"Count\")\n",
    "ax_tenure.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products vs Churn\n",
    "fig_products, ax_products = plt.subplots(figsize=(10, 6))\n",
    "products_churn = pd.crosstab(data_bank[\"products_number\"], data_bank[\"churn\"])\n",
    "products_churn.plot(\n",
    "    kind=\"bar\", ax=ax_products, color=[\"gray\", \"darkred\"], width=0.7\n",
    ")\n",
    "ax_products.set_title(\"Number of Products vs Churn\", fontsize=14)\n",
    "ax_products.set_xlabel(\"Number of Products\")\n",
    "ax_products.set_ylabel(\"Count\")\n",
    "ax_products.set_xticklabels(ax_products.get_xticklabels(), rotation=0)\n",
    "ax_products.legend([\"Stayed (0)\", \"Left (1)\"])\n",
    "ax_products.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot matrix: age, balance, salary\n",
    "fig_scatter, axes_scatter = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Age vs Balance\n",
    "colors = [\"gray\" if c == 0 else \"darkred\" for c in data_bank[\"churn\"]]\n",
    "axes_scatter[0].scatter(\n",
    "    data_bank[\"age\"], data_bank[\"balance\"], c=colors, alpha=0.5, s=20\n",
    ")\n",
    "axes_scatter[0].set_xlabel(\"Age\")\n",
    "axes_scatter[0].set_ylabel(\"Balance\")\n",
    "axes_scatter[0].set_title(\"Age vs Balance\")\n",
    "axes_scatter[0].grid(alpha=0.3)\n",
    "\n",
    "# Age vs Salary\n",
    "axes_scatter[1].scatter(\n",
    "    data_bank[\"age\"], data_bank[\"estimated_salary\"], c=colors, alpha=0.5, s=20\n",
    ")\n",
    "axes_scatter[1].set_xlabel(\"Age\")\n",
    "axes_scatter[1].set_ylabel(\"Estimated Salary\")\n",
    "axes_scatter[1].set_title(\"Age vs Estimated Salary\")\n",
    "axes_scatter[1].grid(alpha=0.3)\n",
    "\n",
    "# Balance vs Salary\n",
    "axes_scatter[2].scatter(\n",
    "    data_bank[\"balance\"],\n",
    "    data_bank[\"estimated_salary\"],\n",
    "    c=colors,\n",
    "    alpha=0.5,\n",
    "    s=20,\n",
    ")\n",
    "axes_scatter[2].set_xlabel(\"Balance\")\n",
    "axes_scatter[2].set_ylabel(\"Estimated Salary\")\n",
    "axes_scatter[2].set_title(\"Balance vs Estimated Salary\")\n",
    "axes_scatter[2].grid(alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"gray\", label=\"Stayed (0)\"),\n",
    "    Patch(facecolor=\"darkred\", label=\"Left (1)\"),\n",
    "]\n",
    "fig_scatter.legend(\n",
    "    handles=legend_elements,\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, 0.02),\n",
    "    ncol=2,\n",
    ")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Creating new features that may improve model performance.\n",
    "\n",
    "### New Feature: balance_to_salary_ratio\n",
    "- Captures the relationship between account balance and income\n",
    "- Customers with high balance relative to salary may have different churn behavior\n",
    "- Formula: `balance / (estimated_salary + 1)` (add 1 to avoid division by zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Feature Created**: `balance_to_salary_ratio`\n",
       "\n",
       "- Min: 0.0000\n",
       "- Max: 9770.8831\n",
       "- Mean: 3.7902\n",
       "- Median: 0.7470\n",
       "\n",
       "This feature captures how much of their salary equivalent a customer keeps in the bank."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create new feature: balance_to_salary_ratio\n",
    "data_bank[\"balance_to_salary_ratio\"] = data_bank[\"balance\"] / (\n",
    "    data_bank[\"estimated_salary\"] + 1\n",
    ")\n",
    "\n",
    "mo.md(f\"\"\"\n",
    "**Feature Created**: `balance_to_salary_ratio`\n",
    "\n",
    "- Min: {data_bank[\"balance_to_salary_ratio\"].min():.4f}\n",
    "- Max: {data_bank[\"balance_to_salary_ratio\"].max():.4f}\n",
    "- Mean: {data_bank[\"balance_to_salary_ratio\"].mean():.4f}\n",
    "- Median: {data_bank[\"balance_to_salary_ratio\"].median():.4f}\n",
    "\n",
    "This feature captures how much of their salary equivalent a customer keeps in the bank.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHCJ",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "Converting categorical variables and splitting into **train/validation/test sets (70/15/15)**.\n",
    "\n",
    "### Why 3-way split instead of 2-way?\n",
    "- **Training set (70%)**: Used to fit models\n",
    "- **Validation set (15%)**: Used for hyperparameter tuning and early stopping (XGBoost)\n",
    "- **Test set (15%)**: Held out until final evaluation — never seen during training or tuning\n",
    "\n",
    "This prevents \"data leakage\" where test set information influences model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to category type\n",
    "data_prepared = data_bank.copy()\n",
    "data_prepared[\"country\"] = pd.Categorical(data_prepared[\"country\"])\n",
    "data_prepared[\"gender\"] = pd.Categorical(data_prepared[\"gender\"])\n",
    "data_prepared[\"credit_card\"] = data_prepared[\"credit_card\"].astype(\"int\")\n",
    "data_prepared[\"active_member\"] = data_prepared[\"active_member\"].astype(\"int\")\n",
    "data_prepared[\"churn\"] = data_prepared[\"churn\"].astype(\n",
    "    \"int\"\n",
    ")  # Ensure int for sklearn\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "data_encoded = pd.get_dummies(\n",
    "    data_prepared, columns=[\"country\", \"gender\"], drop_first=True\n",
    ")\n",
    "\n",
    "# Prepare features and target\n",
    "X = data_encoded.drop(\"churn\", axis=1)\n",
    "y = data_encoded[\"churn\"]\n",
    "\n",
    "# Step 1: Split into train (70%) and temp (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=123, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split temp into validation (15%) and test (15%)\n",
    "# 50% of 30% = 15% each\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=123, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "numerical_features = [\n",
    "    \"credit_score\",\n",
    "    \"age\",\n",
    "    \"tenure\",\n",
    "    \"balance\",\n",
    "    \"estimated_salary\",\n",
    "    \"products_number\",\n",
    "    \"balance_to_salary_ratio\",\n",
    "]\n",
    "\n",
    "# Create scaled versions for models that need it (Logistic Regression, SVM)\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled = X_val.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[numerical_features] = scaler.fit_transform(\n",
    "    X_train[numerical_features]\n",
    ")\n",
    "X_val_scaled[numerical_features] = scaler.transform(X_val[numerical_features])\n",
    "X_test_scaled[numerical_features] = scaler.transform(\n",
    "    X_test[numerical_features]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Data Split Summary (70/15/15):\n",
       "\n",
       "| Set | Samples | Churn=0 | Churn=1 | Churn Rate |\n",
       "|-----|---------|---------|---------|------------|\n",
       "| Train | 7000 | 5574 | 1426 | 20.4% |\n",
       "| Validation | 1500 | 1194 | 306 | 20.4% |\n",
       "| Test | 1500 | 1195 | 305 | 20.3% |\n",
       "\n",
       "**Features**: 12 (including new `balance_to_salary_ratio`)\n",
       "\n",
       "Stratified splitting ensures consistent churn rates across all sets."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.md(f\"\"\"\n",
    "### Data Split Summary (70/15/15):\n",
    "\n",
    "| Set | Samples | Churn=0 | Churn=1 | Churn Rate |\n",
    "|-----|---------|---------|---------|------------|\n",
    "| Train | {len(X_train)} | {(y_train == 0).sum()} | {(y_train == 1).sum()} | {(y_train == 1).sum() / len(y_train) * 100:.1f}% |\n",
    "| Validation | {len(X_val)} | {(y_val == 0).sum()} | {(y_val == 1).sum()} | {(y_val == 1).sum() / len(y_val) * 100:.1f}% |\n",
    "| Test | {len(X_test)} | {(y_test == 0).sum()} | {(y_test == 1).sum()} | {(y_test == 1).sum() / len(y_test) * 100:.1f}% |\n",
    "\n",
    "**Features**: {len(X_train.columns)} (including new `balance_to_salary_ratio`)\n",
    "\n",
    "Stratified splitting ensures consistent churn rates across all sets.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {},
   "source": [
    "## 6. VIF Analysis (Variance Inflation Factor)\n",
    "\n",
    "VIF detects **multicollinearity** — when predictor variables are highly correlated with each other.\n",
    "\n",
    "### Why it matters:\n",
    "- High VIF (>5-10) means a feature is linearly predictable from others\n",
    "- Makes Logistic Regression coefficients unstable and hard to interpret\n",
    "- Tree-based models (Random Forest, XGBoost) are not affected\n",
    "\n",
    "### Interpretation:\n",
    "- **VIF < 5**: Acceptable\n",
    "- **VIF 5-10**: Moderate multicollinearity, investigate\n",
    "- **VIF > 10**: Severe — consider removing or combining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### VIF Results for Numerical Features:\n",
       "\n",
       "| Feature | VIF | Status |\n",
       "|---------|-----|--------|\n",
       "| products_number | 1.10 | OK |\n",
       "| balance | 1.10 | OK |\n",
       "| balance_to_salary_ratio | 1.00 | OK |\n",
       "| estimated_salary | 1.00 | OK |\n",
       "| age | 1.00 | OK |\n",
       "| tenure | 1.00 | OK |\n",
       "| credit_score | 1.00 | OK |\n",
       "\n",
       "\n",
       "**Note**: VIF calculation includes intercept term for accurate multicollinearity detection.\n",
       "High VIF (>10) indicates the feature is highly predictable from other features."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate VIF for numerical features\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "X_vif = X_train[numerical_features].copy()\n",
    "\n",
    "# Handle any infinite or NaN values\n",
    "X_vif = X_vif.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# Add constant for proper VIF calculation\n",
    "X_vif_with_const = add_constant(X_vif)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = numerical_features\n",
    "# Calculate VIF starting from index 1 (skip the constant at index 0)\n",
    "vif_data[\"VIF\"] = [\n",
    "    variance_inflation_factor(X_vif_with_const.values, i + 1)\n",
    "    for i in range(len(numerical_features))\n",
    "]\n",
    "vif_data = vif_data.sort_values(\"VIF\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Add interpretation\n",
    "def vif_interpretation(vif):\n",
    "    if vif < 5:\n",
    "        return \"OK\"\n",
    "    elif vif < 10:\n",
    "        return \"Moderate\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "\n",
    "vif_data[\"Status\"] = vif_data[\"VIF\"].apply(vif_interpretation)\n",
    "\n",
    "mo.md(f\"\"\"\n",
    "### VIF Results for Numerical Features:\n",
    "\n",
    "| Feature | VIF | Status |\n",
    "|---------|-----|--------|\n",
    "{\"\".join([f\"| {row['Feature']} | {row['VIF']:.2f} | {row['Status']} |\" + chr(10) for _, row in vif_data.iterrows()])}\n",
    "\n",
    "**Note**: VIF calculation includes intercept term for accurate multicollinearity detection.\n",
    "High VIF (>10) indicates the feature is highly predictable from other features.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {},
   "source": [
    "## 7. Model Training\n",
    "\n",
    "Training 5 classification models:\n",
    "1. **Random Forest** — bagging ensemble of decision trees\n",
    "2. **AdaBoost** — boosting with adaptive sample weights\n",
    "3. **XGBoost** — gradient boosting with regularization (uses validation set for early stopping)\n",
    "4. **SVM** — support vector machine with RBF kernel\n",
    "5. **Logistic Regression** — linear baseline model\n",
    "\n",
    "All models use `class_weight='balanced'` (or equivalent) to handle class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {},
   "source": [
    "### 7.1 Random Forest\n",
    "\n",
    "Random Forest uses **bagging**: trains many decision trees on bootstrap samples\n",
    "and combines predictions via majority voting. Reduces variance without increasing bias.\n",
    "\n",
    "**Regularization applied**: max_depth=10, min_samples_leaf=20, min_samples_split=50\n",
    "to prevent overfitting (which caused 100% training accuracy in initial runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with regularization to prevent overfitting\n",
    "rf_model = RandomForestClassifier(\n",
    "    random_state=123,\n",
    "    n_estimators=100,\n",
    "    max_depth=10,  # Limit tree depth\n",
    "    min_samples_leaf=20,  # Require at least 20 samples per leaf\n",
    "    min_samples_split=50,  # Require at least 50 samples to split\n",
    "    class_weight=\"balanced\",\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_pred_train = rf_model.predict(X_train)\n",
    "rf_pred_val = rf_model.predict(X_val)\n",
    "rf_pred_test = rf_model.predict(X_test)\n",
    "rf_prob_test = rf_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Random Forest Confusion Matrices**\n",
       "\n",
       "| Set | TN | FP | FN | TP |\n",
       "|-----|----|----|----|----|\n",
       "| Train | 4680 | 894 | 279 | 1147 |\n",
       "| Val | 958 | 236 | 79 | 227 |\n",
       "| Test | 989 | 206 | 92 | 213 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_rf_train = confusion_matrix(y_train, rf_pred_train)\n",
    "cm_rf_val = confusion_matrix(y_val, rf_pred_val)\n",
    "cm_rf_test = confusion_matrix(y_test, rf_pred_test)\n",
    "\n",
    "mo.md(f\"\"\"\n",
    "**Random Forest Confusion Matrices**\n",
    "\n",
    "| Set | TN | FP | FN | TP |\n",
    "|-----|----|----|----|----|\n",
    "| Train | {cm_rf_train[0, 0]} | {cm_rf_train[0, 1]} | {cm_rf_train[1, 0]} | {cm_rf_train[1, 1]} |\n",
    "| Val | {cm_rf_val[0, 0]} | {cm_rf_val[0, 1]} | {cm_rf_val[1, 0]} | {cm_rf_val[1, 1]} |\n",
    "| Test | {cm_rf_test[0, 0]} | {cm_rf_test[0, 1]} | {cm_rf_test[1, 0]} | {cm_rf_test[1, 1]} |\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBYS",
   "metadata": {},
   "source": [
    "### 7.2 AdaBoost\n",
    "\n",
    "AdaBoost is a **boosting** algorithm that trains weak learners sequentially.\n",
    "After each iteration, misclassified samples get higher weights, forcing the next\n",
    "learner to focus on hard cases.\n",
    "\n",
    "**Key difference from XGBoost**: AdaBoost adjusts sample weights; XGBoost uses gradient descent\n",
    "on a loss function with regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "ada_model = AdaBoostClassifier(random_state=123, n_estimators=100)\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "ada_pred_train = ada_model.predict(X_train)\n",
    "ada_pred_val = ada_model.predict(X_val)\n",
    "ada_pred_test = ada_model.predict(X_test)\n",
    "ada_prob_test = ada_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**AdaBoost Confusion Matrices**\n",
       "\n",
       "| Set | TN | FP | FN | TP |\n",
       "|-----|----|----|----|----|\n",
       "| Train | 5366 | 208 | 795 | 631 |\n",
       "| Val | 1146 | 48 | 154 | 152 |\n",
       "| Test | 1132 | 63 | 172 | 133 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_ada_train = confusion_matrix(y_train, ada_pred_train)\n",
    "cm_ada_val = confusion_matrix(y_val, ada_pred_val)\n",
    "cm_ada_test = confusion_matrix(y_test, ada_pred_test)\n",
    "\n",
    "mo.md(f\"\"\"\n",
    "**AdaBoost Confusion Matrices**\n",
    "\n",
    "| Set | TN | FP | FN | TP |\n",
    "|-----|----|----|----|----|\n",
    "| Train | {cm_ada_train[0, 0]} | {cm_ada_train[0, 1]} | {cm_ada_train[1, 0]} | {cm_ada_train[1, 1]} |\n",
    "| Val | {cm_ada_val[0, 0]} | {cm_ada_val[0, 1]} | {cm_ada_val[1, 0]} | {cm_ada_val[1, 1]} |\n",
    "| Test | {cm_ada_test[0, 0]} | {cm_ada_test[0, 1]} | {cm_ada_test[1, 0]} | {cm_ada_test[1, 1]} |\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xXTn",
   "metadata": {},
   "source": [
    "### 7.3 XGBoost\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is a **gradient boosting** algorithm with several advantages:\n",
    "\n",
    "| Feature | AdaBoost | XGBoost |\n",
    "|---------|----------|---------|\n",
    "| Weight update | Sample weights | Gradient descent on loss |\n",
    "| Regularization | None | L1 and L2 penalties |\n",
    "| Missing values | Requires imputation | Handles natively |\n",
    "| Early stopping | Not built-in | Built-in with validation set |\n",
    "| Speed | Slower | Optimized with parallelization |\n",
    "\n",
    "**Why XGBoost often wins**: Regularization prevents overfitting, and early stopping\n",
    "finds optimal number of trees automatically.\n",
    "\n",
    "We use the **validation set** to determine when to stop training (early stopping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with early stopping\n",
    "# Calculate scale_pos_weight for class imbalance\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric=\"auc\",\n",
    "    random_state=123,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=30,\n",
    ")\n",
    "\n",
    "# Fit with validation set for early stopping\n",
    "xgb_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "xgb_pred_train = xgb_model.predict(X_train)\n",
    "xgb_pred_val = xgb_model.predict(X_val)\n",
    "xgb_pred_test = xgb_model.predict(X_test)\n",
    "xgb_prob_test = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "best_iteration = xgb_model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHFh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**XGBoost Confusion Matrices**\n",
       "\n",
       "Early stopping triggered at iteration **154** (out of max 500).\n",
       "This prevents overfitting by stopping when validation performance stops improving.\n",
       "\n",
       "| Set | TN | FP | FN | TP |\n",
       "|-----|----|----|----|----|\n",
       "| Train | 4606 | 968 | 285 | 1141 |\n",
       "| Val | 947 | 247 | 74 | 232 |\n",
       "| Test | 989 | 206 | 68 | 237 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_xgb_train = confusion_matrix(y_train, xgb_pred_train)\n",
    "cm_xgb_val = confusion_matrix(y_val, xgb_pred_val)\n",
    "cm_xgb_test = confusion_matrix(y_test, xgb_pred_test)\n",
    "\n",
    "mo.md(f\"\"\"\n",
    "**XGBoost Confusion Matrices**\n",
    "\n",
    "Early stopping triggered at iteration **{best_iteration}** (out of max 500).\n",
    "This prevents overfitting by stopping when validation performance stops improving.\n",
    "\n",
    "| Set | TN | FP | FN | TP |\n",
    "|-----|----|----|----|----|\n",
    "| Train | {cm_xgb_train[0, 0]} | {cm_xgb_train[0, 1]} | {cm_xgb_train[1, 0]} | {cm_xgb_train[1, 1]} |\n",
    "| Val | {cm_xgb_val[0, 0]} | {cm_xgb_val[0, 1]} | {cm_xgb_val[1, 0]} | {cm_xgb_val[1, 1]} |\n",
    "| Test | {cm_xgb_test[0, 0]} | {cm_xgb_test[0, 1]} | {cm_xgb_test[1, 0]} | {cm_xgb_test[1, 1]} |\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NCOB",
   "metadata": {},
   "source": [
    "### 7.4 SVM (Support Vector Machine)\n",
    "\n",
    "SVM with RBF kernel finds a nonlinear decision boundary by mapping features\n",
    "to a higher-dimensional space.\n",
    "\n",
    "**Important**: SVM requires **scaled features** — we use the StandardScaler-transformed data.\n",
    "\n",
    "**Note**: SVM doesn't provide feature importance directly (unlike tree-based models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqbW",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel (requires scaled features)\n",
    "svm_model = SVC(\n",
    "    kernel=\"rbf\",\n",
    "    C=1.0,\n",
    "    gamma=\"scale\",\n",
    "    class_weight=\"balanced\",\n",
    "    probability=True,  # Needed for predict_proba\n",
    "    random_state=123,\n",
    ")\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "svm_pred_train = svm_model.predict(X_train_scaled)\n",
    "svm_pred_val = svm_model.predict(X_val_scaled)\n",
    "svm_pred_test = svm_model.predict(X_test_scaled)\n",
    "svm_prob_test = svm_model.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SVM Confusion Matrices**\n",
       "\n",
       "| Set | TN | FP | FN | TP |\n",
       "|-----|----|----|----|----|\n",
       "| Train | 4550 | 1024 | 286 | 1140 |\n",
       "| Val | 945 | 249 | 61 | 245 |\n",
       "| Test | 954 | 241 | 79 | 226 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_svm_train = confusion_matrix(y_train, svm_pred_train)\n",
    "cm_svm_val = confusion_matrix(y_val, svm_pred_val)\n",
    "cm_svm_test = confusion_matrix(y_test, svm_pred_test)\n",
    "\n",
    "mo.md(f\"\"\"\n",
    "**SVM Confusion Matrices**\n",
    "\n",
    "| Set | TN | FP | FN | TP |\n",
    "|-----|----|----|----|----|\n",
    "| Train | {cm_svm_train[0, 0]} | {cm_svm_train[0, 1]} | {cm_svm_train[1, 0]} | {cm_svm_train[1, 1]} |\n",
    "| Val | {cm_svm_val[0, 0]} | {cm_svm_val[0, 1]} | {cm_svm_val[1, 0]} | {cm_svm_val[1, 1]} |\n",
    "| Test | {cm_svm_test[0, 0]} | {cm_svm_test[0, 1]} | {cm_svm_test[1, 0]} | {cm_svm_test[1, 1]} |\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TXez",
   "metadata": {},
   "source": [
    "### 7.5 Logistic Regression\n",
    "\n",
    "Linear baseline model. Uses scaled features for stable coefficient estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dNNg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression (requires scaled features)\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=123,\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    ")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "lr_pred_train = lr_model.predict(X_train_scaled)\n",
    "lr_pred_val = lr_model.predict(X_val_scaled)\n",
    "lr_pred_test = lr_model.predict(X_test_scaled)\n",
    "lr_prob_test = lr_model.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCnT",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Logistic Regression Confusion Matrices**\n",
       "\n",
       "| Set | TN | FP | FN | TP |\n",
       "|-----|----|----|----|----|\n",
       "| Train | 4042 | 1532 | 428 | 998 |\n",
       "| Val | 835 | 359 | 96 | 210 |\n",
       "| Test | 864 | 331 | 102 | 203 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_lr_train = confusion_matrix(y_train, lr_pred_train)\n",
    "cm_lr_val = confusion_matrix(y_val, lr_pred_val)\n",
    "cm_lr_test = confusion_matrix(y_test, lr_pred_test)\n",
    "\n",
    "mo.md(f\"\"\"\n",
    "**Logistic Regression Confusion Matrices**\n",
    "\n",
    "| Set | TN | FP | FN | TP |\n",
    "|-----|----|----|----|----|\n",
    "| Train | {cm_lr_train[0, 0]} | {cm_lr_train[0, 1]} | {cm_lr_train[1, 0]} | {cm_lr_train[1, 1]} |\n",
    "| Val | {cm_lr_val[0, 0]} | {cm_lr_val[0, 1]} | {cm_lr_val[1, 0]} | {cm_lr_val[1, 1]} |\n",
    "| Test | {cm_lr_test[0, 0]} | {cm_lr_test[0, 1]} | {cm_lr_test[1, 0]} | {cm_lr_test[1, 1]} |\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wlCL",
   "metadata": {},
   "source": [
    "## 8. Learning Curves (Bias vs Variance Diagnosis)\n",
    "\n",
    "Learning curves show how model performance changes with training set size.\n",
    "\n",
    "### How to interpret:\n",
    "- **High Bias (underfitting)**: Both train and validation scores are low and close together.\n",
    "  Model is too simple, needs more features or complexity.\n",
    "\n",
    "- **High Variance (overfitting)**: Train score is high but validation score is much lower.\n",
    "  Model memorizes training data, needs regularization or more data.\n",
    "\n",
    "- **Good Fit**: Both scores converge at a high value as training size increases.\n",
    "\n",
    "We plot learning curves for **Random Forest** and **XGBoost** to compare their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqZH",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves for Random Forest and XGBoost\n",
    "fig_lc, axes_lc = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cv_lc = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Random Forest learning curve\n",
    "train_sizes_rf, train_scores_rf, val_scores_rf = learning_curve(\n",
    "    rf_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_sizes=train_sizes,\n",
    "    cv=cv_lc,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# XGBoost learning curve (create a fresh model without early stopping for fair comparison)\n",
    "xgb_lc_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n",
    "    random_state=123,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "train_sizes_xgb, train_scores_xgb, val_scores_xgb = learning_curve(\n",
    "    xgb_lc_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_sizes=train_sizes,\n",
    "    cv=cv_lc,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Plot Random Forest\n",
    "axes_lc[0].fill_between(\n",
    "    train_sizes_rf,\n",
    "    train_scores_rf.mean(axis=1) - train_scores_rf.std(axis=1),\n",
    "    train_scores_rf.mean(axis=1) + train_scores_rf.std(axis=1),\n",
    "    alpha=0.2,\n",
    "    color=\"blue\",\n",
    ")\n",
    "axes_lc[0].fill_between(\n",
    "    train_sizes_rf,\n",
    "    val_scores_rf.mean(axis=1) - val_scores_rf.std(axis=1),\n",
    "    val_scores_rf.mean(axis=1) + val_scores_rf.std(axis=1),\n",
    "    alpha=0.2,\n",
    "    color=\"orange\",\n",
    ")\n",
    "axes_lc[0].plot(\n",
    "    train_sizes_rf,\n",
    "    train_scores_rf.mean(axis=1),\n",
    "    \"o-\",\n",
    "    color=\"blue\",\n",
    "    label=\"Train\",\n",
    ")\n",
    "axes_lc[0].plot(\n",
    "    train_sizes_rf,\n",
    "    val_scores_rf.mean(axis=1),\n",
    "    \"o-\",\n",
    "    color=\"orange\",\n",
    "    label=\"Validation\",\n",
    ")\n",
    "axes_lc[0].set_xlabel(\"Training Set Size\")\n",
    "axes_lc[0].set_ylabel(\"ROC-AUC Score\")\n",
    "axes_lc[0].set_title(\"Learning Curve: Random Forest\")\n",
    "axes_lc[0].legend(loc=\"lower right\")\n",
    "axes_lc[0].grid(alpha=0.3)\n",
    "axes_lc[0].set_ylim([0.7, 1.0])\n",
    "\n",
    "# Plot XGBoost\n",
    "axes_lc[1].fill_between(\n",
    "    train_sizes_xgb,\n",
    "    train_scores_xgb.mean(axis=1) - train_scores_xgb.std(axis=1),\n",
    "    train_scores_xgb.mean(axis=1) + train_scores_xgb.std(axis=1),\n",
    "    alpha=0.2,\n",
    "    color=\"blue\",\n",
    ")\n",
    "axes_lc[1].fill_between(\n",
    "    train_sizes_xgb,\n",
    "    val_scores_xgb.mean(axis=1) - val_scores_xgb.std(axis=1),\n",
    "    val_scores_xgb.mean(axis=1) + val_scores_xgb.std(axis=1),\n",
    "    alpha=0.2,\n",
    "    color=\"orange\",\n",
    ")\n",
    "axes_lc[1].plot(\n",
    "    train_sizes_xgb,\n",
    "    train_scores_xgb.mean(axis=1),\n",
    "    \"o-\",\n",
    "    color=\"blue\",\n",
    "    label=\"Train\",\n",
    ")\n",
    "axes_lc[1].plot(\n",
    "    train_sizes_xgb,\n",
    "    val_scores_xgb.mean(axis=1),\n",
    "    \"o-\",\n",
    "    color=\"orange\",\n",
    "    label=\"Validation\",\n",
    ")\n",
    "axes_lc[1].set_xlabel(\"Training Set Size\")\n",
    "axes_lc[1].set_ylabel(\"ROC-AUC Score\")\n",
    "axes_lc[1].set_title(\"Learning Curve: XGBoost\")\n",
    "axes_lc[1].legend(loc=\"lower right\")\n",
    "axes_lc[1].grid(alpha=0.3)\n",
    "axes_lc[1].set_ylim([0.7, 1.0])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wAgl",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Learning Curve Analysis\n",
       "\n",
       "| Model | Final Train AUC | Final Val AUC | Gap (Train - Val) | Diagnosis |\n",
       "|-------|-----------------|---------------|-------------------|-----------|\n",
       "| Random Forest | 0.909 | 0.850 | 0.059 | High variance |\n",
       "| XGBoost | 0.890 | 0.857 | 0.033 | OK |\n",
       "\n",
       "**Interpretation**:\n",
       "- A large gap (>0.05) between train and validation indicates **overfitting** (high variance)\n",
       "- If both curves are low and flat, the model has **high bias** (underfitting)\n",
       "- Converging curves at high values indicate a **good fit**\n",
       "\n",
       "The shaded regions show +/- 1 standard deviation across CV folds."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interpret learning curves\n",
    "rf_gap = train_scores_rf.mean(axis=1)[-1] - val_scores_rf.mean(axis=1)[-1]\n",
    "xgb_gap = train_scores_xgb.mean(axis=1)[-1] - val_scores_xgb.mean(axis=1)[-1]\n",
    "\n",
    "rf_final_val = val_scores_rf.mean(axis=1)[-1]\n",
    "xgb_final_val = val_scores_xgb.mean(axis=1)[-1]\n",
    "\n",
    "mo.md(f\"\"\"\n",
    "### Learning Curve Analysis\n",
    "\n",
    "| Model | Final Train AUC | Final Val AUC | Gap (Train - Val) | Diagnosis |\n",
    "|-------|-----------------|---------------|-------------------|-----------|\n",
    "| Random Forest | {train_scores_rf.mean(axis=1)[-1]:.3f} | {rf_final_val:.3f} | {rf_gap:.3f} | {\"High variance\" if rf_gap > 0.05 else \"OK\"} |\n",
    "| XGBoost | {train_scores_xgb.mean(axis=1)[-1]:.3f} | {xgb_final_val:.3f} | {xgb_gap:.3f} | {\"High variance\" if xgb_gap > 0.05 else \"OK\"} |\n",
    "\n",
    "**Interpretation**:\n",
    "- A large gap (>0.05) between train and validation indicates **overfitting** (high variance)\n",
    "- If both curves are low and flat, the model has **high bias** (underfitting)\n",
    "- Converging curves at high values indicate a **good fit**\n",
    "\n",
    "The shaded regions show +/- 1 standard deviation across CV folds.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rEll",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis\n",
    "\n",
    "Comparing which features matter most to Random Forest vs XGBoost.\n",
    "\n",
    "**Note**: SVM and Logistic Regression don't provide direct feature importance\n",
    "(though LR coefficients can be interpreted with caution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dGlV",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance comparison\n",
    "fig_imp, axes_imp = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Random Forest importance\n",
    "rf_importance = pd.DataFrame(\n",
    "    {\"feature\": X_train.columns, \"importance\": rf_model.feature_importances_}\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# XGBoost importance\n",
    "xgb_importance = pd.DataFrame(\n",
    "    {\"feature\": X_train.columns, \"importance\": xgb_model.feature_importances_}\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Plot RF\n",
    "sns.barplot(\n",
    "    data=rf_importance.head(10),\n",
    "    x=\"importance\",\n",
    "    y=\"feature\",\n",
    "    hue=\"feature\",\n",
    "    palette=\"Blues_r\",\n",
    "    legend=False,\n",
    "    ax=axes_imp[0],\n",
    ")\n",
    "axes_imp[0].set_title(\"Random Forest: Top 10 Features\", fontsize=12)\n",
    "axes_imp[0].set_xlabel(\"Importance\")\n",
    "axes_imp[0].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Plot XGBoost\n",
    "sns.barplot(\n",
    "    data=xgb_importance.head(10),\n",
    "    x=\"importance\",\n",
    "    y=\"feature\",\n",
    "    hue=\"feature\",\n",
    "    palette=\"Oranges_r\",\n",
    "    legend=False,\n",
    "    ax=axes_imp[1],\n",
    ")\n",
    "axes_imp[1].set_title(\"XGBoost: Top 10 Features\", fontsize=12)\n",
    "axes_imp[1].set_xlabel(\"Importance\")\n",
    "axes_imp[1].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SdmI",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Top 5 Features by Model\n",
       "\n",
       "| Rank | Random Forest | XGBoost |\n",
       "|------|---------------|---------|\n",
       "| 1 | age (0.383) | products_number (0.211) |\n",
       "| 2 | products_number (0.196) | age (0.185) |\n",
       "| 3 | balance (0.089) | active_member (0.170) |\n",
       "| 4 | balance_to_salary_ratio (0.062) | country_Germany (0.110) |\n",
       "| 5 | active_member (0.058) | gender_Male (0.074) |\n",
       "\n",
       "**Business Insight**: The most important features for predicting churn are likely\n",
       "age, balance, and number of products — consistent with the original analysis.\n",
       "\n",
       "Check if `balance_to_salary_ratio` appears in top features — if so, the new feature adds value."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mo.md(f\"\"\"\n",
    "### Top 5 Features by Model\n",
    "\n",
    "| Rank | Random Forest | XGBoost |\n",
    "|------|---------------|---------|\n",
    "| 1 | {rf_importance.iloc[0][\"feature\"]} ({rf_importance.iloc[0][\"importance\"]:.3f}) | {xgb_importance.iloc[0][\"feature\"]} ({xgb_importance.iloc[0][\"importance\"]:.3f}) |\n",
    "| 2 | {rf_importance.iloc[1][\"feature\"]} ({rf_importance.iloc[1][\"importance\"]:.3f}) | {xgb_importance.iloc[1][\"feature\"]} ({xgb_importance.iloc[1][\"importance\"]:.3f}) |\n",
    "| 3 | {rf_importance.iloc[2][\"feature\"]} ({rf_importance.iloc[2][\"importance\"]:.3f}) | {xgb_importance.iloc[2][\"feature\"]} ({xgb_importance.iloc[2][\"importance\"]:.3f}) |\n",
    "| 4 | {rf_importance.iloc[3][\"feature\"]} ({rf_importance.iloc[3][\"importance\"]:.3f}) | {xgb_importance.iloc[3][\"feature\"]} ({xgb_importance.iloc[3][\"importance\"]:.3f}) |\n",
    "| 5 | {rf_importance.iloc[4][\"feature\"]} ({rf_importance.iloc[4][\"importance\"]:.3f}) | {xgb_importance.iloc[4][\"feature\"]} ({xgb_importance.iloc[4][\"importance\"]:.3f}) |\n",
    "\n",
    "**Business Insight**: The most important features for predicting churn are likely\n",
    "age, balance, and number of products — consistent with the original analysis.\n",
    "\n",
    "Check if `balance_to_salary_ratio` appears in top features — if so, the new feature adds value.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgWD",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation Metrics\n",
    "\n",
    "Calculating comprehensive metrics on the **test set** (final evaluation).\n",
    "\n",
    "### Metrics:\n",
    "- **Accuracy**: Overall correct predictions\n",
    "- **Sensitivity (Recall)**: Correctly identified churners (TP / (TP + FN))\n",
    "- **Specificity**: Correctly identified non-churners (TN / (TN + FP))\n",
    "- **Precision**: Accuracy of positive predictions (TP / (TP + FP))\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **AUC**: Area under ROC curve\n",
    "- **Gini**: 2 * AUC - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yOPj",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\"><thead><tr style=\"text-align: right;\"><th></th><th>Random Forest</th><th>AdaBoost</th><th>XGBoost</th><th>SVM</th><th>Logistic Reg</th></tr></thead><tbody><tr><th>Accuracy</th><td>0.8013</td><td>0.8433</td><td>0.8173</td><td>0.7867</td><td>0.7113</td></tr><tr><th>Sensitivity</th><td>0.6984</td><td>0.4361</td><td>0.7770</td><td>0.7410</td><td>0.6656</td></tr><tr><th>Specificity</th><td>0.8276</td><td>0.9473</td><td>0.8276</td><td>0.7983</td><td>0.7230</td></tr><tr><th>Precision</th><td>0.5084</td><td>0.6786</td><td>0.5350</td><td>0.4839</td><td>0.3801</td></tr><tr><th>F1-Score</th><td>0.5884</td><td>0.5309</td><td>0.6337</td><td>0.5855</td><td>0.4839</td></tr><tr><th>AUC</th><td>0.8487</td><td>0.8521</td><td>0.8743</td><td>0.8545</td><td>0.7510</td></tr><tr><th>Gini</th><td>0.6975</td><td>0.7042</td><td>0.7486</td><td>0.7091</td><td>0.5021</td></tr></tbody></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate metrics for all models\n",
    "def calc_metrics(y_true, y_pred, y_prob, cm):\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Sensitivity\": recall_score(y_true, y_pred, pos_label=1),\n",
    "        \"Specificity\": cm[0, 0] / (cm[0, 0] + cm[0, 1]),\n",
    "        \"Precision\": precision_score(y_true, y_pred, pos_label=1),\n",
    "        \"F1-Score\": f1_score(y_true, y_pred, pos_label=1),\n",
    "        \"AUC\": auc,\n",
    "        \"Gini\": 2 * auc - 1,\n",
    "    }\n",
    "\n",
    "\n",
    "metrics_rf = calc_metrics(y_test, rf_pred_test, rf_prob_test, cm_rf_test)\n",
    "metrics_ada = calc_metrics(y_test, ada_pred_test, ada_prob_test, cm_ada_test)\n",
    "metrics_xgb = calc_metrics(y_test, xgb_pred_test, xgb_prob_test, cm_xgb_test)\n",
    "metrics_svm = calc_metrics(y_test, svm_pred_test, svm_prob_test, cm_svm_test)\n",
    "metrics_lr = calc_metrics(y_test, lr_pred_test, lr_prob_test, cm_lr_test)\n",
    "\n",
    "# Create comparison table\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Random Forest\": metrics_rf,\n",
    "        \"AdaBoost\": metrics_ada,\n",
    "        \"XGBoost\": metrics_xgb,\n",
    "        \"SVM\": metrics_svm,\n",
    "        \"Logistic Reg\": metrics_lr,\n",
    "    }\n",
    ").round(4)\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fwwy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Best Model per Metric (Test Set)\n",
       "\n",
       "| Metric | Best Model | Value |\n",
       "|--------|------------|-------|\n",
       "| Accuracy | AdaBoost | 0.8433 |\n",
       "| Sensitivity | XGBoost | 0.7770 |\n",
       "| Specificity | AdaBoost | 0.9473 |\n",
       "| F1-Score | XGBoost | 0.6337 |\n",
       "| AUC | XGBoost | 0.8743 |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find best model for each metric\n",
    "best_models = metrics_df.idxmax(axis=1)\n",
    "\n",
    "mo.md(f\"\"\"\n",
    "### Best Model per Metric (Test Set)\n",
    "\n",
    "| Metric | Best Model | Value |\n",
    "|--------|------------|-------|\n",
    "| Accuracy | {best_models[\"Accuracy\"]} | {metrics_df.loc[\"Accuracy\", best_models[\"Accuracy\"]]:.4f} |\n",
    "| Sensitivity | {best_models[\"Sensitivity\"]} | {metrics_df.loc[\"Sensitivity\", best_models[\"Sensitivity\"]]:.4f} |\n",
    "| Specificity | {best_models[\"Specificity\"]} | {metrics_df.loc[\"Specificity\", best_models[\"Specificity\"]]:.4f} |\n",
    "| F1-Score | {best_models[\"F1-Score\"]} | {metrics_df.loc[\"F1-Score\", best_models[\"F1-Score\"]]:.4f} |\n",
    "| AUC | {best_models[\"AUC\"]} | {metrics_df.loc[\"AUC\", best_models[\"AUC\"]]:.4f} |\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LJZf",
   "metadata": {},
   "source": [
    "## 11. ROC Curves\n",
    "\n",
    "Comparing all models using ROC curves on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urSm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves for all models\n",
    "fig_roc, ax_roc = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "models_roc = {\n",
    "    \"Random Forest\": rf_prob_test,\n",
    "    \"AdaBoost\": ada_prob_test,\n",
    "    \"XGBoost\": xgb_prob_test,\n",
    "    \"SVM\": svm_prob_test,\n",
    "    \"Logistic Regression\": lr_prob_test,\n",
    "}\n",
    "\n",
    "colors_roc = [\"blue\", \"red\", \"green\", \"purple\", \"orange\"]\n",
    "\n",
    "for (name, probs), color in zip(models_roc.items(), colors_roc):\n",
    "    fpr, tpr, _ = roc_curve(y_test, probs, pos_label=1)\n",
    "    auc = roc_auc_score(y_test, probs)\n",
    "    ax_roc.plot(\n",
    "        fpr, tpr, color=color, linewidth=2, label=f\"{name} (AUC={auc:.3f})\"\n",
    "    )\n",
    "\n",
    "ax_roc.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"Random (AUC=0.500)\")\n",
    "ax_roc.set_xlabel(\"False Positive Rate\", fontsize=12)\n",
    "ax_roc.set_ylabel(\"True Positive Rate\", fontsize=12)\n",
    "ax_roc.set_title(\"ROC Curves Comparison (Test Set)\", fontsize=14)\n",
    "ax_roc.legend(loc=\"lower right\")\n",
    "ax_roc.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jxvo",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "### Summary of Enhancements\n",
    "\n",
    "| Enhancement | Purpose | Finding |\n",
    "|-------------|---------|---------|\n",
    "| **70/15/15 Split** | Proper validation for early stopping and model selection | Prevents data leakage; consistent 20% churn rate |\n",
    "| **XGBoost** | Better performance through gradient boosting with regularization | Best AUC (0.87) and Sensitivity (0.78) |\n",
    "| **SVM** | Non-linear decision boundary via RBF kernel | Good sensitivity (0.74), comparable to XGBoost |\n",
    "| **RF Regularization** | Prevent overfitting (was 100% train accuracy) | Fixed with max_depth=10, min_samples constraints |\n",
    "| **VIF with Intercept** | Accurate multicollinearity detection | Fixed suspicious VIF values by adding constant term |\n",
    "| **Learning Curves** | Diagnose bias vs variance | XGBoost shows healthy gap (0.03), RF was overfitting |\n",
    "\n",
    "### XGBoost vs AdaBoost: Key Differences\n",
    "\n",
    "Both are boosting methods, but:\n",
    "- **AdaBoost**: Adjusts sample weights; simple and fast; prone to overfitting on noisy data\n",
    "- **XGBoost**: Uses gradient descent on loss function; has L1/L2 regularization; handles missing values; supports early stopping\n",
    "\n",
    "XGBoost typically achieves better performance because regularization prevents overfitting."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
